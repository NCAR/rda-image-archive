{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setting up an \"acceptance test\" by cleaning out the `01_files` workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf 01_files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create `01_files` workspace and populate it with `tar` archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We create a working directory for this example.\n",
    "os.makedirs(\"01_files\", exist_ok=True)\n",
    "\n",
    "# We create a \"data\" subdirectory for the images and the metadata tag files.\n",
    "# os.makedirs(\"01_files/data\", exist_ok=True)\n",
    "data = \"01_files/data\"\n",
    "\n",
    "# We create an \"out\" subdirectory for the processed images and the metadata catalog.\n",
    "os.makedirs(\"01_files/out\", exist_ok=True)\n",
    "out  = \"01_files/out\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# import logging\n",
    "# import http.client\n",
    "\n",
    "# # To set up logging.\n",
    "# # https://stackoverflow.com/questions/16337511/\n",
    "# http.client.HTTPConnection.debuglevel = 1\n",
    "# logging.basicConfig()\n",
    "# logging.getLogger().setLevel(logging.DEBUG)\n",
    "# requests_log = logging.getLogger(\"requests.packages.urllib3\")\n",
    "# requests_log.setLevel(logging.DEBUG)\n",
    "# requests_log.propagate = True\n",
    "\n",
    "# To access \n",
    "mwe_url = \"https://github.com/coltongrainger/2020-02-05-mwe/raw/master/2020-02-05-mwe.tar.gz\"\n",
    "mwe_tar_archive = \"01_files/2020-02-05-mwe.tar.gz\"\n",
    "res = requests.get(mwe_url)\n",
    "if res.status_code == 200:\n",
    "    with open(mwe_tar_archive, 'wb') as f:\n",
    "        f.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "retcode = subprocess.call(['tar', '-xvf', mwe_tar_archive, '-C', \"01_files\"])\n",
    "if retcode == 0:\n",
    "    print(\"Extracted successfully\")\n",
    "else:\n",
    "    raise IOError('tar exited with code %d' % retcode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(mwe_tar_archive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in os.listdir(data):\n",
    "    print(os.listdir(os.path.join(data, p)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## catalog images under `data` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i \"scripts/utils.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We generate a fixed sequence for uuids.\n",
    "get_fixed_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/glade/u/home/rdadata/lib/python/site-packages')\n",
    "\n",
    "# get_exiftool()\n",
    "import subprocess\n",
    "import os\n",
    "repo_dir = subprocess.Popen(['git', 'rev-parse', '--show-toplevel'], stdout=subprocess.PIPE).communicate()[0].rstrip().decode('utf-8')\n",
    "sys.path.append(os.path.join(repo_dir, \"dependencies/pyexiftool\"))\n",
    "import exiftool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_catalog = get_normalized_catalog(data)\n",
    "# We generate a metadata catalog (unnormalized) from the data directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = unnormalize_catalog(normalized_catalog)\n",
    "# We flatten the normalized catalog. \n",
    "# Each file in the data directory \"has its own entry\" in this catalog.\n",
    "# We'll eventually ignore non-image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_timestamped_catalog(catalog, out)\n",
    "# We write this version of the metadata catalog to the output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalog = read_timestamped_catalog(out)\n",
    "# We read in the most recent version of the metadata catalog from the out directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rename images by `uuid`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elementary_family = [c for c in catalog if c['media_type'].startswith(\"image\")]\n",
    "# We create a list of all the entries in the catalog that are image files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# We'll perform some file renames between the data directory and the out directory.\n",
    "\n",
    "# We move all the images in the catalog to the output directory.\n",
    "for member in elementary_family:\n",
    "    os.rename(member['file_path'], os.path.join(out, member['uuid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pick up catalog with `pandas`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(catalog)\n",
    "df = df[df['media_type'].str.contains(\"image\")] \n",
    "# we only want to keep track of image files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## filter `DataFrame` for archives, platforms, documents, and images to insert into DB `images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_df = df.filter(regex=(\"^archive\"))\n",
    "arc_df = arc_df.drop_duplicates()\n",
    "arc_df.rename(columns=lambda x: re.sub('archive.','',x), inplace=True)\n",
    "arc_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt_df = df.filter(regex=(\"^platform\"))\n",
    "plt_df = plt_df.drop_duplicates()\n",
    "plt_df.rename(columns=lambda x: re.sub('platform.','',x), inplace=True)\n",
    "plt_df.to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_df = pd.concat(\n",
    "    [df.filter(regex=(\"^document\")), \n",
    "     df.filter(items=[\"archive.host_country\", \"archive.name\"]),\n",
    "     df.filter(items=[\"platform.host_country\", \"platform.name\"])\n",
    "    ], axis=1\n",
    ").drop_duplicates()\n",
    "doc_df.rename(columns=lambda x: re.sub('document.', '', x), inplace=True)\n",
    "doc_df.fillna(\"\", inplace=True) # avoid NaNs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## declare and persist tables in `images`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run -i scripts/tables.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import create_engine\n",
    "# engine = create_engine('mysql+mysqlconnector://user:pass@rda-db.ucar.edu/images')\n",
    "# engine = create_engine('mysql+pymysql://user:pass@localhost/images')\n",
    "# TODO read defaults extra file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.drop_all(engine) # clean out the DB\n",
    "metadata.create_all(engine) # reinitialize the canonical schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = engine.connect() # let's start working with these tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata insertion for archives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import insert, select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = insert(archive)\n",
    "rp = connection.execute(ins, arc_df.to_dict('records'))\n",
    "# throws integrity error if run twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = select([archive])\n",
    "rp = connection.execute(s)\n",
    "for arc in rp:\n",
    "    print(arc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata insertion for platforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ins = insert(platform)\n",
    "rp = connection.execute(ins, plt_df.to_dict('records'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = select([platform])\n",
    "rp = connection.execute(s)\n",
    "for plt in rp:\n",
    "    print(plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata insertion for documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import and_\n",
    "\n",
    "def get_archive_fk(doc_dict): # add error handling\n",
    "    s = select([archive.c.archive_id])\n",
    "    s = s.where(and_(\n",
    "        archive.c.name == doc_dict['archive.name'],\n",
    "        archive.c.host_country == doc_dict['archive.host_country']\n",
    "    ))\n",
    "    s = s.limit(1) # should be unique anyways\n",
    "    rp = connection.execute(s)\n",
    "    result = rp.scalar() # is the parent id\n",
    "    return result\n",
    "\n",
    "def get_platform_fk(doc_dict): # add error handling\n",
    "    s = select([platform.c.platform_id])\n",
    "    s = s.where(and_(\n",
    "        platform.c.name == doc_dict['platform.name'],\n",
    "        platform.c.host_country == doc_dict['platform.host_country']\n",
    "    ))\n",
    "    s = s.limit(1) # should be unique anyways\n",
    "    rp = connection.execute(s)\n",
    "    result = rp.scalar() # is the parent id\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc_dict in doc_df.to_dict('record'):\n",
    "    arc_id = get_archive_fk(doc_dict)\n",
    "    plt_id = get_platform_fk(doc_dict)\n",
    "    for key in [\n",
    "        'archive.host_country',\n",
    "        'archive.name',\n",
    "        'platform.host_country',\n",
    "        'platform.name'\n",
    "    ]:\n",
    "        doc_dict.pop(key)\n",
    "    ins = insert(document)\n",
    "    rp = connection.execute(ins,\n",
    "            doc_dict,\n",
    "            archive_id = arc_id,\n",
    "            platform_id = plt_id\n",
    "    )\n",
    "# throws an integrity error if run twice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = select([document.c.id_within_archive_type, document.c.id_within_archive, document.c.contact_person])\n",
    "s = s.where(document.c.start_date.between(\"1900-01-01\", \"2000-01-01\"))\n",
    "rp = connection.execute(s)\n",
    "for res in rp:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = select([document.c.id_within_archive_type, document.c.id_within_archive, document.c.contact_person])\n",
    "s = s.where(document.c.start_date.between(\"1800-01-01\", \"1900-01-01\"))\n",
    "rp = connection.execute(s)\n",
    "for res in rp:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## metadata insertion for images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_document_fk(img_dict):\n",
    "    s = select([document.c.document_id])\n",
    "    s = s.where(and_(\n",
    "        document.c.id_within_archive == img_dict['document.id_within_archive'],\n",
    "        document.c.id_within_archive_type == img_dict['document.id_within_archive_type']\n",
    "    ))\n",
    "    s = s.limit(1) # should be unique anyways\n",
    "    rp = connection.execute(s)\n",
    "    result = rp.scalar() # is the parent id\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_df = df.filter(items=[\"uuid\",\"media_type\",\"document.id_within_archive\",\"document.id_within_archive_type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_dict in img_df.to_dict('record'):\n",
    "    doc_id = get_document_fk(img_dict)\n",
    "    for key in [\"document.id_within_archive\",\"document.id_within_archive_type\"]:\n",
    "        img_dict.pop(key)\n",
    "    img_dict['document_id'] = doc_id\n",
    "    ins = insert(image, img_dict)\n",
    "    print(ins.compile().params)\n",
    "    rp = connection.execute(ins, img_dict)    \n",
    "# throws an integrity error if run twice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieval of images by uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "s = select([image.c.uuid, image.c.media_type])\n",
    "s = s.order_by(image.c.uuid)\n",
    "rp = connection.execute(s)\n",
    "\n",
    "for img in rp:\n",
    "    print(os.path.join(out, img.uuid), img.media_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## subsetting by date range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = [image.c.uuid, image.c.media_type, \n",
    "           document.c.start_date, document.c.standardized_region_list]\n",
    "\n",
    "twentieth_century = select(columns)\n",
    "twentieth_century = twentieth_century.select_from(\n",
    "    image.join(document)).where(\n",
    "    document.c.start_date.between(\"1900-01-01\", \"1999-12-31\"))\n",
    "\n",
    "rp = connection.execute(twentieth_century).fetchall()\n",
    "\n",
    "for img in rp:\n",
    "    display(Image(\n",
    "        filename=os.path.join(out, img.uuid),\n",
    "        format=img.media_type.replace(\"image/\",\"\")\n",
    "    ))\n",
    "    for key in img.keys():\n",
    "        print('{:>20}: {}'.format(key, img[key]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns = [image.c.uuid, image.c.media_type, \n",
    "           document.c.start_date, document.c.standardized_region_list]\n",
    "\n",
    "nineteenth_century = select(columns)\n",
    "nineteenth_century = nineteenth_century.select_from(\n",
    "    image.join(document)).where(\n",
    "    document.c.start_date.between(\"1800-01-01\", \"1899-12-31\"))\n",
    "nineteenth_century = nineteenth_century.order_by(document.c.start_date)\n",
    "\n",
    "rp = connection.execute(nineteenth_century).fetchall()\n",
    "\n",
    "for img in rp:\n",
    "    display(Image(\n",
    "        filename=os.path.join(out, img.uuid),\n",
    "        format=img.media_type.replace(\"image/\",\"\")\n",
    "    ))\n",
    "    for key in img.keys():\n",
    "        print('{:>20}: {}'.format(key, img[key]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
